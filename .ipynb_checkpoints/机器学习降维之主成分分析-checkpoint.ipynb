{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主成分基本思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>主成分基本思想：</b>在主成分分析中，首先对给定数据进行规范化，使得数据每一个变量的平均值维0，方差为1，之后对数据进行正交变换，原来由线性相关变量表示的数据，通过正交变换变成由若干个线性无关的新变量表示的数据。新变量是可能的正交变换中变量的方差的和最大的，方差表示子新变量上信息的大小，将新变量依次称为第一主成分，第二主成分等\n",
    "\n",
    "通过主成分分析，可以利用主成分近似地表示原始数据，这可理解为发现数据的'基本结构'，也可以把数据由少数主成分表示，这可理解为数据降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总体主成分定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$假设X = {(x_1,x_2,x_3,...,x_m)}^T是m维随机变量，其均值向量为\\mu$,$$\\mu = E(X) = {(\\mu_1,\\mu_2,...,\\mu_m)}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$协方差矩阵是\\xi$,$$\\xi = cov(x_i,x_j) = E[(x_i-\\mu){(x_j-\\mu)}^T]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$考虑由m维随机变量x到m维随机变量y = {(y_1,y_2,...,y_m)}^T的线性变换$\n",
    "$$y_i = a_i^TX = a_{1i}x_1+a_{2i}x_2+...+a_{mi}x_m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$a_i^T = (a_{1i},a_{2i},...,a_{mi})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>由随机变量的性质可以知道：</b>\n",
    "$$E(y_i) = a_{i}^T\\mu$$\n",
    "$$var(y_i) = a_i^T\\xi a_i$$\n",
    "$$cov(y_i,y_j) = a_i^T\\xi a_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面给出总体主成分的定义\n",
    "\n",
    "<b>定义(总体主成分):</b>给定一个上面$y_i = a_i^TX = a_{1i}x_1+a_{2i}x_2+...+a_{mi}x_m$的线性变换，如果满足下列条件：\n",
    "- (1)系数向量$a_i^T是单位向量，即a_i^T a_i = 1$\n",
    "- (2)$变量y_i与y_j互不相关，即它们的协方差为0$\n",
    "- (3)$变量y_1是X的所有线性变换中方差最大的;y_2是与y_1不相关的X的所有线性变换中方差最大的;$ $一般地y_i是与y_1,y_2,...,y_{i-1}都不相关的X的所有线性变换中方差最大的;$ $这时分别称y_1,y_2,...,y_m为X的第一主成分、第二主成分、...、第m主成分$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 样本均值和方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设对m维随机变量$X={(x_1,x_2,...,x_m)}^T$进行n次独立观测，$x_1,x_2,...,x_n$表示观测样本，其中$x_j={(x_{1j},x_{2j},...,x_{mj})}^T$表示第j个观测样本，$x_{ij}表示第j个观测样本的第i个变量$\n",
    "\n",
    "给定样本矩阵X,可以估计样本均值，以及样本协方差，样本均值向量$$\\tilde x = \\frac{1}{n}\\sum_{j=1}^nx_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本方差$$S = \\frac{1}{n-1}\\sum_{j=1}^n(x_{ik} - \\tilde x_i)(x_{jk}-\\tilde j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 样本方差推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本方差公式$$S = \\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\mu_i)^2$$\n",
    "扩展开来得到$$S = \\frac{1}{n-1}[(X-\\frac{1}{n}X^TI_nI_n^T)^T(X-\\frac{1}{n}X^TI_nI_n^T)]$$\n",
    "$$S = \\frac{1}{n-1}X^T(I_n - \\frac{1}{n}I_nI_n^T)(I_n - \\frac{1}{n}I_nI_n^T)X$$\n",
    "令$H = I_n - \\frac{1}{n}I_nI_n^T$得$$S = \\frac{1}{n-1}X^THX$$\n",
    "其中H为等幂矩阵HH=H和中心矩阵$H_n*I_n = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA求解流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1)数据归一化，均值为0,方差为1\n",
    "- (2)计算协方差矩阵\n",
    "- (3)计算协方差矩阵的特征值和特征向量\n",
    "- (4)将特征值从大到小排序\n",
    "- (5)保留最上面的N个特征向量\n",
    "- (6)将数据转换到上述N个特征向量构建的新空间中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python实现PCA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def pca(dataMat, topNfeat=9999999):\n",
    "    meanVals = mean(dataMat, axis=0)\n",
    "    meanRemoved = dataMat - meanVals #remove mean\n",
    "    covMat = cov(meanRemoved, rowvar=0)\n",
    "    eigVals,eigVects = linalg.eig(mat(covMat))\n",
    "    eigValInd = argsort(eigVals)            #sort, sort goes smallest to largest\n",
    "    eigValInd = eigValInd[:-(topNfeat+1):-1]  #cut off unwanted dimensions\n",
    "    redEigVects = eigVects[:,eigValInd]       #reorganize eig vects largest to smallest\n",
    "    lowDDataMat = meanRemoved * redEigVects#transform data into new dimensions\n",
    "    reconMat = (lowDDataMat * redEigVects.T) + meanVals\n",
    "    return lowDDataMat, reconMat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA最小平方误差理论推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA求解其实是寻找最佳投影方向，即多个方向的标准正交基构成一个超平面。\n",
    "\n",
    "理论思想：在高维空间中，我们实际上是要找到一个d维超平面，使得数据点到这个超平面的距离平方和最小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设$x_k$表示p维空间的k个点，$z_k$表示$x_k$在超平面D上的投影向量，$W = {w_1,w_2,...,w_d}$为D维空间的标准正交基，即PCA最小平方误差理论转换为如下优化问题$$z_k = \\sum_{i=1}^d (w_i^T x_k)w_i---(1)$$\n",
    "$$argmin \\sum_{i=1}^k||x_k - z_k||_2^2$$\n",
    "$$s.t. w_i^Tw_j = p(当i==j时p=1,否则p=0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：$w_i^Tx_k$为x_k在w_i基向量的投影长度，$w_i^Tx_kw_i$为w_i基向量的坐标值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求解：\n",
    "\n",
    "$L = (x_k - z_k)^T(x_k-z_k)$\n",
    "\n",
    "$L= x_k^Tx_k - x_k^Tz_k - z_k^Tx_k + z_k^Tz_k$\n",
    "\n",
    "由于向量内积性质$x_k^Tz_k = z_k^Tx_k$\n",
    "\n",
    "$L = x_k^Tx_k - 2x_k^Tz_k + z_k^Tz_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将(1)带入得$$x_k^Tz_k = \\sum_{i=1}^dw_i^Tx_kx_k^Tw_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_k^Tz_k = \\sum_{i=1}^d\\sum_{j=1}^d(w_i^Tx_kw_i)^T(w_j^Tx_kw_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据约束条件s.t.得$$z_k^Tz_k = \\sum_{i=1}^dw_i^Tx_k^Tx_kw_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L =x_k^Tx_k - \\sum_{i=1}^dw_i^Tx_kx_k^Tw_i$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据奇异值分解$$\\sum_{i=1}^dw_i^Tx_kx_k^Tw_i = tr(W^Tx_k^Tx_kW)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L =argmin\\sum_{i=1}^kx_k^Tx_k - tr(W^Tx_k^Tx_kW) = argmin\\sum_{i=1}^k- tr(W^Tx_k^Tx_kW) + C$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "等价于带约束得优化问题：$$argmaxtr(W^TXX^TW)$$\n",
    "$$s.t. W^TW = I$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最佳超平面W与最大方差法求解的最佳投影方向一致，即协方差矩阵的最大特征值所对应的特征向量，差别仅是协方差矩阵$\\xi$的一个倍数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$argmin\\phi(W,Z|X) = tr((X-W^TZ)^T(X-W^TZ)) = ||X-W^TZ||_F^2$$\n",
    "$$s.t.W^TW=I_q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：X为(n,p),Z为(n,q),q < p,w为(p,q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该定理表达的意思也就是平方差理论，将降维后的矩阵通过W^T投影回去，再与X计算最小平方差，值越小说明信息损失越少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\phi$目标函数最小时，W为X的前q个特征向量矩阵且$Z=W^TX$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上优化可以通过拉格朗日对偶问题求得，最终也会得到$$argmaxtr(W^TXX^TW)$$\n",
    "$$s.t. W^TW = I$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核PCA推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>核函数:</b>设X是输入空间($R^n$的子集或离散子集),又F为特征空间(希尔伯特空间)，如果存在一个从X到F的隐射$$\\phi (X):X -> F$$使得对所有x,z\\in X,函数K(x,z)满足条件$$K(x,z) = \\phi (x)\\bullet \\phi (z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面推导F投影到的主成分定义的平面，根据F样本方差的特征值分解得(为推导方便去掉前面的($\\frac{1}{n-1}$)$$F^THFV_i = \\lambda _i V_i$$由于H为等逆矩阵，则$$F^THHFV_i = \\lambda _i V_i$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于想得到F很难，我们换一种思路将求F转移求K上，根据AA^T与A^TA的关系：非零特质值相同，得到$$HFF^THU_i = \\lambda _iU_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两边同时乘以$F^TH$得到$$F^THHFF^THU_i = \\lambda _iF^THU_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上式可以得到$F^THU_i$为$F^THHF$的特征向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将$F^THU_i$进行归一化$$U_{normal} = \\frac{F^THU_i}{{||U_i^THFF^THU_i||}_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于$HFF^TH = HKH = \\lambda _i$,则$$U_{normal} = \\lambda ^{-\\frac{1}{2}}F^THU_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F投影到$U_normal$定义的平面$$P = F_{center} U_{normal}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P= (F-\\frac{1}{n}\\sum_{i=1}^nF_i)(\\lambda ^{-\\frac{1}{2}}F^THU_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P= (F-\\frac{1}{n}F^TI_n)(\\lambda ^{-\\frac{1}{2}}F^THU_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P= \\lambda ^{-\\frac{1}{2}}(K - \\frac{1}{n}K(x,x_i))HU_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附：奇异值分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奇异值分解是一个能适用于任意矩阵的一种分解方法：$$A = U\\xi{V}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设A是一个M*N的矩阵，那么U就是M*M的方阵（里面的向量是正交的，U里面向量为左奇异向量），$\\xi$为M*N的实数对角矩阵（对角线以外的元素都是0，对角线上的元素为奇异值），\n",
    "$V^T$是一个N*N的矩阵（里面的向量是正交的，V里面的向量称为右奇异向量）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再结合特征值分解：$$(A^T\\bullet{A})\\bullet{V_i} = \\lambda{_i}\\bullet{V_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面得到的$V_i$就是奇异值分解种的右奇异向量，$\\lambda{_i}$为特征值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外我们还可以得到：$$\\sigma{_i} = \\sqrt{\\lambda{_i}}\\\\u_i=\\frac{1}{\\sigma{_i}}AV_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的$\\sigma{_i}为奇异值$，$u_i$为左奇异向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见的做法是将奇异值由大到小排列，在大多数情况下，前10%甚至1%的奇异值的和就占了全部奇异值和的99%以上，也就是说我们可以用前r大的奇异值来近似描述矩阵\n",
    "$$A_{m\\times{n}}\\approx{U_{m\\times{r}}\\xi{_{r\\times{r}}}V_{r\\times{n}}^T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r是一个远小于m,n的数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资料:\n",
    "- (1)李航老师的<统计学习方法>\n",
    "- (2)<机器学习实战基于Scikit-Learn和TensorFlow>\n",
    "- (3)<百面机器学习>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "203.99px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
