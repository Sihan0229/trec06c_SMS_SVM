{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文主要是学习笔记，一方面是为了加强理解，感觉在做笔记过程中理解起来更简单，另一方面为了加强记忆，建立大脑关于‘隐马尔可夫模型’的神经网络，最后当然是为了后面查看方便"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型场景"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在介绍隐马尔可夫模型之前先来看个例子：\n",
    "假设有4个盒子，每个盒子里面都装有红、白两种颜色的求，盒子里面的红包球数量如下：\n",
    "                                \n",
    "                           盒子\n",
    "            1           2         3      4\n",
    "\n",
    "红球数      5           3         6      8\n",
    "白球数      5           7         4      2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照下面的方式抽球，产生一个球的颜色的观测序列：\n",
    "- (1)开始，从4个盒子里以等概率随机选取一个盒子，从这个盒子里随机抽出一个球，记录其颜色，然后放回\n",
    "- (2)然后，从当前盒子随机转移到下一个盒子，规则是：如果当前盒子是盒子1，那么下一个盒子一定是盒子2，如果当前盒子是盒子2或3，那么分别以概率0.4和0.6转移到左边或右边的盒子，如果当前是盒子4，那么各以0.5的概率停留在盒子4或转移到盒子3\n",
    "- (3)确定转移的盒子后，再从这个盒子里随机抽出一个球，记录其颜色，放回\n",
    "- (4)如此下去，重复进行5次，得到一个球的颜色的观测序列：$$O = (红，红，白，白，红)$$\n",
    "\n",
    "在这个过程中，观察者只能观测到球的颜色的序列，观测不到球是从哪个盒子取出的，即观测不到盒子的序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隐马尔可夫模型三要素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的例子是一个典型的<b>隐马尔可夫模型</b>。有两个随机序列，一个是盒子的序列(状态序列)，一个是球的颜色的观测序列，前者是隐藏的，只有后者是可观测的。\n",
    "\n",
    "隐马尔可夫模型有三要素，表示为$$\\lambda = (A, B, \\pi)$$\n",
    "注：A为状态转移矩阵，B为观测概率分布矩阵，$\\pi 为初始状态概率向量$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的例子，来分别计算下A,B和$\\pi$的值\n",
    "\n",
    "<b>状态转移概率分布矩阵：</b>\n",
    "$$ A = \n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   0   & 1 &   0 & 0 \\\\\n",
    "   0.4 & 0 & 0.4 & 0 \\\\\n",
    "   0 & 0.4 &   0 & 0.6 \\\\\n",
    "   0 &   0 & 0.5 & 0.5\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$$\n",
    "$A[ij]$表示从状态i转移到状态j的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>观测概率分布矩阵：</b>\n",
    "\n",
    "$$ B = \n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   0.5 & 0.5 \\\\\n",
    "   0.3 & 0.7 \\\\\n",
    "   0.6 & 0.4 \\\\\n",
    "   0.8 & 0.2\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$$\n",
    "$B[i0]$表示盒子i中取出红球的概率，$B[i1]$表示盒子i中取出白球的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>初始概率分布：</b>\n",
    "$$\\pi = (0.25,0.25,0.25,0.25)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隐马尔可夫模型的三个基本问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 概率计算问题\n",
    "\n",
    "给定模型$\\lambda = (A,B,\\pi)$和观测序列$O = (o_1,o_2,...,0_T)$,计算在模型$\\lambda$下观测模型出现的概率$P(O|\\lambda)$\n",
    "\n",
    "(2) 学习问题\n",
    "\n",
    "已知观测序列$O = (o_1,o_2,...,0_T)$,估计模型$\\lambda = (A,B,\\pi)$参数，使得在该模型下观测序列概率$P(O|\\lambda)$最大，用极大似然估计的方法估计参数\n",
    "\n",
    "(3) 预测问题，也称为解码问题\n",
    "\n",
    "已知模型$\\lambda = (A,B,\\pi)$和观测序列$O = (o_1,o_2,...,0_T)$，求对给定观测序列条件概率$P(O|\\lambda)$最大的状态序列$I = (i_1,i_2,...,i_T)$。即给定观测序列，求最有可能的对应的状态序列\n",
    "\n",
    "下面分别介绍针对不同问题的解决算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概率计算算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定模型$\\lambda = (A,B,\\pi)$和观测序列$O = (o_1,o_2,...,0_T)$,计算在模型$\\lambda$下观测模型出现的概率$P(O|\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>(1) 计算状态t1下观测为红球的情况,注：序列和矩阵索引都从1开始</b>\n",
    "\n",
    "第一次从盒子1选择红球的情况：\n",
    "$$a_1(1) = \\pi_1 B_1(o_1) = 0.25 * 0.5 = 0.125$$\n",
    "第一次从盒子2选择红球的情况：\n",
    "$$a_1(2) = \\pi_2 B_2(o_1) = 0.25 * 0.3 = 0.075$$\n",
    "第一次从盒子3选择红球的情况：\n",
    "$$a_1(3) = \\pi_3 B_3(o_1) = 0.25 * 0.6 = 0.15$$\n",
    "第一次从盒子4选择红球的情况：\n",
    "$$a_1(4) = \\pi_4 B_4(o_1) = 0.25 * 0.8 = 0.20$$\n",
    "\n",
    "<b>(2) 计算状态t2下观测为红球的情况，及第二次选择为红球的情况</b>\n",
    "\n",
    "第二次从盒子1选择红球的情况：\n",
    "$$a_2(1) = a_1(1)A_{11}B_1(o_2) + a_1(2)A_{21}B_1(o_2) + + a_1(3)A_{31}B_1(o_2) + + a_1(4)A_{41}B_1(o_2)$$\n",
    "第二次从盒子2选择红球的情况：\n",
    "$$a_2(2) = a_1(1)A_{12}B_2(o_2) + a_1(2)A_{22}B_2(o_2) + + a_1(3)A_{32}B_2(o_2) + + a_1(4)A_{42}B_2(o_2)$$\n",
    "第二次从盒子3选择红球的情况：\n",
    "$$a_2(3) = a_1(1)A_{13}B_3(o_2) + a_1(2)A_{23}B_3(o_2) + + a_1(3)A_{33}B_3(o_2) + + a_1(4)A_{43}B_3(o_2)$$\n",
    "第二次从盒子4选择红球的情况：\n",
    "$$a_2(4) = a_1(1)A_{14}B_4(o_2) + a_1(2)A_{24}B_4(o_2) + + a_1(3)A_{34}B_4(o_2) + + a_1(4)A_{44}B_4(o_2)$$\n",
    "\n",
    "...\n",
    "\n",
    "<b>通过上述规律我们得到公式:</b>\n",
    "\n",
    "(1) 初值\n",
    "$$a_1(i) = \\pi(i)B_i(o_1)$$\n",
    "(2) 递推\n",
    "$$a_{t+1}(i) = [\\sum_{j=1}^N a_t(j)A_{ji}]B_i(o_{t+1}) $$\n",
    "(3) 终止\n",
    "$$P(O|\\lambda) = \\sum_{i=1}^N a_T(i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后向算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "顾名思义，后向算法就是根据t时刻的观测序列概率算出t-1时刻观测序列的概率\n",
    "\n",
    "令在t时刻状态为$q_i$的条件下，从t+1到T的观测序列的概率为$\\beta_t(i)$,则$$\\beta_t(i) = P(o_{t+1}，o_{t+2},...,o_T|i_t=q_i,\\lambda)$$\n",
    "\n",
    "<b>要特别注意$\\beta_t(i)$的定义，后面才能很好的理解</b>\n",
    "\n",
    "(1) 对最终时刻的所有状态$q_i$规定$$\\beta_T(i) = 1$$\n",
    "\n",
    "(2) $$\\beta_t(i) = \\sum_{j=1}^N a_{ij}b_j(0_{t+1})\\beta_{t+1}(j)$$\n",
    "\n",
    "(3) $$P(O|\\lambda) = \\sum_{i=1}^N\\pi_ib_i(o_1)\\beta_1(i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已知观测序列$O = (o_1,o_2,...,0_T)$,估计模型$\\lambda = (A,B,\\pi)$参数，使得在该模型下观测序列概率$P(O|\\lambda)$最大，用极大似然估计的方法估计参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐马尔可夫模型的学习，根据训练数据集是包括观测序列和对应的状态序列还是只有观测序列，可以分别由监督学习与无监督学习实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于监督学习，由于数据集包含了观测序列和对应的状态序列，这样就可以直接根据利用数据集预估模型参数\n",
    "\n",
    "对于非监督学习，可以使用EM算对隐参数进行学习。EM算法参考附录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已知模型$\\lambda = (A,B,\\pi)$和观测序列$O = (o_1,o_2,...,0_T)$，求对给定观测序列条件概率$P(O|\\lambda)$最大的状态序列$I = (i_1,i_2,...,i_T)$。即给定观测序列，求最有可能的对应的状态序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 维特比算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "维特比算法实际是用动态规划解隐马尔可夫模型预测问题，即用动态规划求概率最大路径\n",
    "\n",
    "定义两个变量：\n",
    "\n",
    "$\\delta_t(i)$表示在时刻t状态为i的所有单个路径中的最大概率值\n",
    "$$\\delta_t(i) = max P(i_t=i,i_{t-1},...,i_1,o_t,...,o_1|\\lambda), i = 1,2,...,N$$\n",
    "\n",
    "$\\psi_t(i)$表示在时刻t状态为i的所有单个路径中概率最大的路径的第t-1个节点\n",
    "$$\\psi_t(i) = arg max_{1<=j<=N} [\\delta_{t-1}(j)a_{ji}],i = 1,2,...,N$$\n",
    "\n",
    "(1) 初始化$$\\delta_1(i) = \\pi_ib_i(o_1)$$\n",
    "$$\\psi_1(i) = 0$$\n",
    "\n",
    "(2) 递推，对t=2,3,...,T\n",
    "$$\\delta_t(i) = max_{1<=j<=N} [\\delta_{t-1}(j)a_{ji}]b_i(o_t)$$\n",
    "$$\\psi_t(i) = arg max_{1<=j<=N} [\\delta_{t-1}(j)a_{ji}]$$\n",
    "\n",
    "(3) 终止\n",
    "$$P^* = max_{1<=i<=N}\\delta_T(i)$$\n",
    "$$i_T^* = arg max_{1<=i<=N} [\\delta_T(i)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附：EM算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM算法定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：观测变量数据X，隐变量数据Z,联合分布$P(X,Z|\\theta)$,也称为完全数据，这样更好理解点\n",
    "\n",
    "输出：模型参数$\\theta$\n",
    "\n",
    "(1)选择初始模型参数$\\theta^{(0)}$，开始迭代\n",
    "\n",
    "(2)E步：记$\\theta^{i}$为第i次迭代参数$\\theta$的估计值，计算在第i次迭代的期望$$Q(\\theta,\\theta^{(i)}) = E(logP(x,z|\\theta)|x,\\theta^{(i)}))=\\int_zlogp(x,z|\\theta)p(z|\\theta^{(i)})$$\n",
    "(3)M步：求使$\\theta^{(i+1)} = Q(\\theta,\\theta^{(i)})的最大值$\n",
    "\n",
    "(4)重复第(2)步和第(3)步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM算法几点说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)参数的初值可以任意选择，但需注意EM算法对初始值是敏感的\n",
    "\n",
    "(2)E步求$Q(\\theta,\\theta^{(i)})$,Q函数中的Z是为隐变量，X是观测数据，$Q(\\theta,\\theta^{(i)})$中的第一个变元表示要极大化的参数，第二个变元表示参数的当前估计值，每次迭代实际在求Q的极大值\n",
    "\n",
    "(3)给出停止迭代的条件，一般是对较小的正数$\\xi_i,\\xi_2$,若满足$||\\theta^{(i+1)} - \\theta^{(i)} < \\xi_i||或||Q(\\theta^{(i+1)},\\theta^{(i)})-Q(\\theta^{(i)},\\theta^{(i)})|| < \\xi_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM算法推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(\\theta)= argmaxlogP(x|\\theta) = argmaxlog\\int_zp(x,z|\\theta)dz$$\n",
    "$$L(\\theta) = argmaxlog\\int_z\\frac{p(x,z|\\theta)}{p(z|\\theta^{(i)})}p(z|\\theta^{(i)})dz$$\n",
    "由于log函数为凹函数，则$$L(\\theta) \\geq \\int_zlog\\frac{p(x,z|\\theta)}{p(z|\\theta^{(i)})}p(z|\\theta^{(i)})dz$$\n",
    "$$L(\\theta) \\geq \\int_zlogp(x,z|\\theta)p(z|\\theta^{(i)})dz - \\int_zlog(p(z|\\theta^{(i)}))p(z|\\theta^{(i)})dz$$\n",
    "由于减式后面与模型参数$\\theta$无关，$P(z|\\theta^{(i)})是已知的$，所以只需关注减式前面的式子，令$$Q(\\theta,\\theta^{(i)})=\\int_zlogp(x,z|\\theta)p(z|\\theta^{(i)})$$\n",
    "和算法定义中的步骤(2)相同，将原L的优化问题转换为求原问题下界$Q(\\theta,\\theta^{(i)})$的最大值\n",
    "因此，任何可以使$Q(\\theta,\\theta^{(i)})$增大的$\\theta$都可以使$L(\\theta)$增大,为了使$L(\\theta)$有尽可能的增长，选择使$Q(\\theta,\\theta^{(i)})$达到最大，即$$\\theta^{(i+1)} = argmaxQ(\\theta,\\theta^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM算法收敛性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>定理1</b>：$设P(x|\\theta)为观测数据的似然函数，\\theta^{(i)}为EM算法得到的参数估计序列，P(x|\\theta^{(i)})为对应的似然函数序列，则P(x|\\theta^{(i)})单调递增$\n",
    "<b>定理2</b>：$设L(\\theta) = logP(x|\\theta)为观测数据的似然函数，\\theta^{(i)}为EM算法得到的参数估计序列，L(\\theta^{(i)})为对应的似然函数序列$\n",
    "\n",
    "(1)$如果P(x|\\theta)有上界，则L(\\theta^{(i)})收敛到某一值L^*$\n",
    "(2)$在函数Q(\\theta,\\theta^{(i)})与L(\\theta)满足一定条件下，由EM算法得到的参数估计序列\\theta^{(i)}的收敛值\\theta^*是L(\\theta)的稳定值$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>以上为EM算法的'官方'说明，若不理解可以参考博客https://www.jianshu.com/p/1121509ac1dc<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后针对隐马尔可夫模型抛出抛出两个问题：\n",
    "\n",
    "(1) 如何对中文分词问题用隐马尔可夫模型进行建模和训练？\n",
    "\n",
    "(2) 最大熵马尔可夫模型为什么会产生标注偏置问题？如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>参考资料：</b>\n",
    "李航老师的《统计学习方法》"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244.93px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
