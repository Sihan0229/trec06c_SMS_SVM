{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑斯蒂分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设X是连续随机变量，X服从逻辑斯蒂分布是指X具有下列分布函数和密度函数：\n",
    "$$F(x) = P(X <= x) = \\frac{1}{1+\\exp^{-(x-u)/\\lambda}}$$\n",
    "$$f(x) = F'(x) = \\frac{\\exp^{-(x-u)/\\lambda}}{\\lambda (1+e^{-(x-u)/\\lambda})^2}$$\n",
    "式中，u为位置参数，$\\lambda > 0$为形状参数，分布函数属于逻辑斯蒂函数，其图形是一条S型曲线，该曲线以$(u,\\frac{1}{2})$为中心对称"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二项逻辑斯蒂回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二项逻辑斯蒂回归模型是一种分类模型，由条件概率p(y|x)表示，形式未参数化的逻辑斯蒂分布，这里的变量X为实数，随机变量y取值为1或0，逻辑斯蒂模型条件概率分布如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y=1|x) = \\frac{\\exp(w{\\bullet}x+b)}{1+\\exp(w{\\bullet}x+b)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y=0|x) = \\frac{1}{1+\\exp(w{\\bullet}x+b)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设$$p(y = 1|x) = \\theta(x),p(y=0|x) = 1 - \\theta(x)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数：$$L(\\theta(x)) = \\prod_{i=1}^N[\\theta(x_i)]^{y_i}[1-\\theta(x_i)]^{1-y_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数似然函数：$$L(\\theta(x)) = \\sum_{i=1}^Ny_i * \\log\\theta(x_i)+(1-y_i)\\log(1-\\theta(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求$L(\\theta(x))$的极大值，得到w的估计值,由于$L(\\theta(x))$为凸函数，可以直接求损失函数的一阶偏导："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\delta{L}}{\\delta{w_j}} = \\sum_{i=1}^N[y_i*\\frac{1}{\\theta(x_i)} - (1-y_i)*\\frac{1}{1-\\theta(x_i)}] *\\frac{\\delta{\\theta(x)}}{\\delta{w_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于$\\frac{\\delta{\\theta(x)}}{\\delta{w}} = \\theta(x_i) * (1 - \\theta(x_i))*x_j^i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到：$$\\frac{\\delta{L}}{\\delta{w_j}} = \\sum_{i=1}^N(y_i-\\theta(x_i))*x_j^i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多项逻辑斯蒂回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二项逻辑斯蒂回归模型是二项分类模型，用于二类分类，多项逻辑斯蒂回归模型用于多类分类，假设离散型随机变量Y的取值集合是{1，2，...,K},那么多项逻辑斯蒂回归模型是：$$P(Y=k|x) = \\frac{\\exp(w_k\\bullet x)}{1+\\sum_{k=1}^{K-1}\\exp(w_k\\bullet x)}$$\n",
    "$$P(Y=K|x) = \\frac{1}{1+\\sum_{k=1}^{K-1}\\exp(w_k\\bullet x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：k=1,2,...,K-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大熵模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是最大熵原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件，在没有更多信息的情况下，那些不确定的部分是“等可能的”。\n",
    "\n",
    "'不要将所有鸡蛋放在一个篮子里面'这句话，用最大熵原理来理解就是将鸡蛋分散放在多个篮子中的受益和放同一个篮子的收益是一样的，反而减少了风险损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最大熵模型的学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于给定的训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_n,y_n)}$以及特征函数$f_i(x,y),i=1,2,...,n$,最大熵模型的学习等价于约束最优化问题：\n",
    "$$max \\space H(P) = -\\sum_{x,y}\\tilde P(x)P(y|x)\\log P(y|x)$$\n",
    "$$s.t \\space E_p(f_i) = E_{\\tilde p}(f_i), \\space i = 1,2,...,n$$\n",
    "$$\\sum_yP(y|x) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：\n",
    "$$E_{\\tilde p}(f) = \\sum_{x,y}\\tilde P(x,y)f(x,y)为特征函数f(x,y)关于经验分布\\tilde p(X,Y)的期望值$$\n",
    "$$E_p(f) = \\sum_{x,y}\\tilde P(x)P(y|x)f(x,y)是特征函数f(x,y)关于模型P(Y|X)与经验分布\\tilde P(X)的期望值$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过拉格朗日对偶问题求解：\n",
    "$$P_w(y|x) = \\frac{1}{Z_w(x)}\\exp (\\sum_{i=1}^nw_if_i(x,y))$$\n",
    "$$Z_w(x) = \\sum_{y}\\exp(\\sum_{i=1}^nw_if_i(x,y))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn逻辑回归实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression类参数说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>penalty</b>: str,'l1'or'l2',default-'l2'</li>\n",
    "    <li><b>dual</b>: bool,default=False,对偶只用于线性回归的l2范数</li>\n",
    "    <li><b>tol</b>: 提前停止标准，float,default=1e-4</li>\n",
    "    <li><b>C</b>: float,default=1.0,惩罚项系数</li>\n",
    "    <li><b>fit_intercept</b>: boo,default=True</li>\n",
    "    <li><b>intercept_scaling</b>: </li>\n",
    "    <li><b>class_weight</b>: </li>\n",
    "    <li><b>random_state</b>: </li>\n",
    "    <li><b>solver</b>: 优化算法方案，str,{'newton-cg','lbfs','liblinear','sag','saga'},default='liblinear'</li>\n",
    "    <li><b>max_iter</b>: </li>\n",
    "    <li><b>multi_class</b>: str,{'ovr','multinomial','auto'},default=''ovr，ovr即one-vs-rest(OvR)，而multinomial即many-vs-many(MvM)。如果是二元逻辑回归，ovr和multinomial并没有任何区别，区别主要在多元逻辑回归上。 OvR的思想很简单，无论你是多少元逻辑回归，我们都可以看做二元逻辑回归。具体做法是，对于第K类的分类决策，我们把所有第K类的样本作为正例，除了第K类样本以外的所有样本都作为负例，然后在上面做二元逻辑回归，得到第K类的分类模型。其他类的分类模型获得以此类推。而MvM则相对复杂，这里举MvM的特例one-vs-one(OvO)作讲解。如果模型有T类，我们每次在所有的T类样本里面选择两类样本出来，不妨记为T1类和T2类，把所有的输出为T1和T2的样本放在一起，把T1作为正例，T2作为负例，进行二元逻辑回归，得到模型参数。我们一共需要T(T-1)/2次分类。可以看出OvR相对简单，但分类效果相对略差（这里指大多数样本分布情况，某些样本分布下OvR可能更好）。而MvM分类相对精确，但是分类速度没有OvR快。如果选择了ovr，则4种损失函数的优化方法liblinear，newton-cg,lbfgs和sag都可以选择。但是如果选择了multinomial,则只能选择newton-cg, lbfgs和sag了。</li>\n",
    "    <li><b>verbose</b>: </li>\n",
    "    <li><b>warm_start</b>: </li>\n",
    "    <li><b>n_jobs</b>: </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "252.39px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
