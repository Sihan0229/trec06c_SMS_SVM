{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">EM算法是一种迭代算法，传说中的上帝算法。用以含有隐变量的概率模型参数的极大似然估计，或极大后验概率估计。EM算法在概率图模型中应用广泛，下面将对EM算法推导、算法收敛性及EM算法在高斯混合模型中的应用进行总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 场景描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "李航老师的<统计学习方法>中列举了以下例子：\n",
    "假设有3枚硬币，分别记作A,B,C。这些硬币正面出现的概率分别是$\\pi,p和q$。进行如下掷硬币试验，先掷A，根据起结果再选出硬币B或C，正面选硬币B,反面选硬币C;然后掷的硬币，掷硬币的结果，出现正面记作1，出现反面记作0；独立地重复n次试验(这里n=10),观测结果如下:$$1,1,0,1,0,0,1,0,1,1$$\n",
    "假设只能观测到掷硬币的结果，不能观测到掷硬币的过程\n",
    "\n",
    "<b>问1:</b>在$\\pi,p,q$已知的情况下，计算上述观测出现的最大概率\n",
    "\n",
    "<b>问2:</b>在$\\pi,p,q$未知的情况下，仅根据观测序列估算三枚银币正面出现的概率，即求$(\\pi,p,q)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求观测序列出现的最大概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1出现的概率为:$$p(y_i = 1) = \\pi p  + (1- \\pi)q$$\n",
    "0出现的概率为:$$p(y_i = 0) = \\pi (1-p) + (1 - \\pi)(1-q)$$\n",
    "整理为:$$p(y_i) = \\pi p ^{y_i}{(1-p)}^{(1-y_i)} + (1-\\pi)q^{y_i}{(1-q)}^{1-y_i}$$\n",
    "观测序列极大似然估计:$$p(Y) = \\prod_{i=1}^np(y_i) = \\prod_{i=1}^n\\pi p ^{y_i}{(1-p)}^{(1-y_i)} + (1-\\pi)q^{y_i}{(1-q)}^{(1-y_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求模型$\\theta = (\\pi,p,q)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑求模型参数$\\theta = (\\pi,p,q)$的极大似然估计，即$$\\widehat{\\theta} = argmax\\log P(Y|\\theta)$$,这个问题没有解析解，只有通过迭代的方法求。EM算法就是可以用于求解这个问题的迭代算法，总结完EM算法推导过程后再回过头来通过EM算法估计模型参数$\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM算法定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：观测变量数据X，隐变量数据Z,联合分布$P(X,Z|\\theta)$,也称为完全数据，这样更好理解点\n",
    "\n",
    "输出：模型参数$\\theta$\n",
    "\n",
    "(1)选择初始模型参数$\\theta^{(0)}$，开始迭代\n",
    "\n",
    "(2)E步：记$\\theta^{i}$为第i次迭代参数$\\theta$的估计值，计算在第i次迭代的期望$$Q(\\theta,\\theta^{(i)}) = E(logP(x,z|\\theta)|x,\\theta^{(i)}))=\\int_zlogp(x,z|\\theta)p(z|\\theta^{(i)})$$\n",
    "(3)M步：求使$\\theta^{(i+1)} = Q(\\theta,\\theta^{(i)})的最大值$\n",
    "\n",
    "(4)重复第(2)步和第(3)步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM算法几点说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)参数的初值可以任意选择，但需注意EM算法对初始值是敏感的\n",
    "\n",
    "(2)E步求$Q(\\theta,\\theta^{(i)})$,Q函数中的Z是为隐变量，X是观测数据，$Q(\\theta,\\theta^{(i)})$中的第一个变元表示要极大化的参数，第二个变元表示参数的当前估计值，每次迭代实际在求Q的极大值\n",
    "\n",
    "(3)给出停止迭代的条件，一般是对较小的正数$\\xi_i,\\xi_2$,若满足$||\\theta^{(i+1)} - \\theta^{(i)} < \\xi_i||或||Q(\\theta^{(i+1)},\\theta^{(i)})-Q(\\theta^{(i)},\\theta^{(i)})|| < \\xi_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM算法推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(\\theta)= argmaxlogP(x|\\theta) = argmaxlog\\int_zp(x,z|\\theta)dz$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(\\theta) = argmaxlog\\int_z\\frac{p(x,z|\\theta)}{p(z|\\theta^{(i)})}p(z|\\theta^{(i)})dz$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于log函数为凹函数，则$$L(\\theta) \\geq \\int_zlog\\frac{p(x,z|\\theta)}{p(z|\\theta^{(i)})}p(z|\\theta^{(i)})dz$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(\\theta) \\geq \\int_zlogp(x,z|\\theta)p(z|\\theta^{(i)})dz - \\int_zlog(p(z|\\theta^{(i)}))p(z|\\theta^{(i)})dz$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于减式后面与模型参数$\\theta$无关，$P(z|\\theta^{(i)})是已知的$，所以只需关注减式前面的式子，令$$Q(\\theta,\\theta^{(i)})=\\int_zlogp(x,z|\\theta)p(z|\\theta^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和算法定义中的步骤(2)相同，将原L的优化问题转换为求原问题下界$Q(\\theta,\\theta^{(i)})$的最大值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，任何可以使$Q(\\theta,\\theta^{(i)})$增大的$\\theta$都可以使$L(\\theta)$增大,为了使$L(\\theta)$有尽可能的增长，选择使$Q(\\theta,\\theta^{(i)})$达到最大，即$$\\theta^{(i+1)} = argmaxQ(\\theta,\\theta^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM算法收敛性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>定理1</b>：$设P(x|\\theta)为观测数据的似然函数，\\theta^{(i)}为EM算法得到的参数估计序列，P(x|\\theta^{(i)})为对应的似然函数序列，则P(x|\\theta^{(i)})单调递增$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>定理2</b>：$设L(\\theta) = logP(x|\\theta)为观测数据的似然函数，\\theta^{(i)}为EM算法得到的参数估计序列，L(\\theta^{(i)})为对应的似然函数序列$\n",
    "\n",
    "(1)$如果P(x|\\theta)有上界，则L(\\theta^{(i)})收敛到某一值L^*$\n",
    "(2)$在函数Q(\\theta,\\theta^{(i)})与L(\\theta)满足一定条件下，由EM算法得到的参数估计序列\\theta^{(i)}的收敛值\\theta^*是L(\\theta)的稳定值$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附：求场景描述中的模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
